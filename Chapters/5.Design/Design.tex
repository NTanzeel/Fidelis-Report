\chapter{Design}
\label{Chapter:Design}
With a clear identification of the problem, sufficient motivations, and well defined requirements, based on informed research a design incorporating all the details could be developed. Although the design process is being carried out before the implementation phase, it is important to note that these are only initial designs. Due to the agile methodology the design and implementation processes are essentially interleaved with one another. This allows for design of one component to be completed, which can then be implemented and improved by feedback. The design of the next component will be influenced by the feedback and lessons learnt from the development of the previous components.

The first part of the design process involved coming up with an overall system architecture, which is discussed in section 5.1. However the first component to be designed would be the database as this is required by almost all other components in the system and should be readily available. In order to develop the database, a level of data collection was also necessary which is briefly touched on throughout this section.

The remainder of the design section covers the user interface design in depth for each of the significant pages on the system. The consistency of the user interface as well as design choices such as colours and layouts are also discussed. Finally a brief overview of responsive mobile design is provided.

\section{System Design and Architecture}
The systems overall layered design and architecture can be seen in figures \ref{fig:LayerArchitecture} and \ref{fig:SystemArchitecture} respectively. The former provides a high level overview of all the components in the system and the interaction between these components using various technologies. The latter, however, provides an in depth view of the data flow and the processes in the system.

\subsection{High Level Design}
The diagram in figure \ref{fig:LayerArchitecture} shows a high level overview of the system design. There are three main layers of the system which make up the web application. These are the User Interface (UI) layer developed using HTML 5 and CSS3, the Application Services (AS) layer which performs the bulk of the logic in PHP, and the DataBase Management System (DBMS) layer which allows the storage and retrieval of data using MySQL. Unfortunately, the AS layer will not be enough to provide tailored content as PHP is not powerful enough to process such large quantities of data. For this reason, an additional layer, namely `background', broken down into two smaller layers for data collection and data analysis, is introduced. This layer will be responsible for running analysis algorithms as schedules jobs that will analyse the data and update the database, allowing the AS layer to output the processed data.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{Images/Design/LayerArchitecture}
  \caption{High Level System Design} \label{fig:LayerArchitecture} 
\end{figure}

\subsection{Overall System Architecture}
The operation of the system is broken down into five stages: data collection, data processing, recommendation, content filtering and user interface. The functionality of each of these stages is implemented across multiple layer of the high level system design in figure \ref{fig:LayerArchitecture}. The data collection stage will primarily be responsible for collecting data about the user, such as their personal, as well as capturing the content, text or photos, posted by the user and storing this in the database so that it may be processed and visualised later. This raw data is represented by the black lines with arrows indicating the direction of the data flow. Additionally, the data collection process will also collect data such as tags, categories and possibly tweets from twitter and store this so that it may be used for training. This stage can be extended to collect data from other sources such as new feeds for better categorisation and trend plotting. The stored posts then undergo processing to assign tags explicitly used by the author but at the same time also assign tags detected by the algorithm based on the content of the post. Once the post has been tagged, the post is assigned to a category based on its tags. The processed data, represented by green lines, is then stored back in the database with state fields which indicate that the tuples have been processed. Once the data is all in a standard format and has been correctly associated, based on the viewers settings, the appropriate recommendation algorithm is ran which outputs recommended posts for each user. The recommended data, represented by red lines, is now held in a separate data storage so that it can quickly be fetched by the server and presented to the user. However, before we can present this data to the user, we need to filter it as it may contain abusive content. This process is carried out by the content filtering stage which looks at the abuse score assigned to each post and filters out abusive posts. The abuse score is computed by a logistic regression model, in the data analysis stage in figure \ref{fig:LayerArchitecture}, trained on content from twitter collected in the data collection stage. The original posts storage is updated at regular intervals by a scheduled job. Finally, the filtered content, represented by blue lines, can be displayed to the user on various pages and widgets throughout the site. 

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{Images/Design/SystemArchitecture}
  \caption{Proposed System Architecture Diagram} \label{fig:SystemArchitecture} 
\end{figure}

\section{Data Collection}

\subsection{Training Data}
Both abuse detection and tag categorisation use supervised machine learning techniques in order to classify posts.  Therefore, data must be collected with which the machine learning models can be trained. Because the data will be immutable, storing the training data in Comma-Separated Values (CSV) files will be sufficient. In addition, this file format is easily compatible for reading into the Python scripts which will build the model.

As part of their competition `Detecting Insults in Social Commentary', Kaggle provided CSV datasets containing sample messages alongside a boolean attribute, representing whether that post is determined to be abusive or not ~\cite{Kaggle:Dataset}. This data is publicly available and the messages are in the same format as Fidelis posts would be expected to be, making it suitable for use when training the model.

Whereas the Kaggle have curated a clean dataset which can be readily used in training the abuse detection, a dataset containing sample posts alongside their category was not available. However, Zubiaga and Ji have published a dataset containing Tweet IDs with their corresponding topic tag ~\cite{Zubiaga:Tweets}. For this dataset to be usable in this project when predicting the category of posts on the social network, further data must be collected from the Twitter API so that the text of the Tweet can be determined, since it is the text which contains the features from which the model can learn.

\subsection{Template Data}


\subsection{User Authentication}
Manually

\section{Data Processing}
Processing data in Fidelis plays a key role in meeting the requirements set by the stakeholders. Data processing will occur through abuse detection on posts to remove abusive and derogatory content from the site, content filtering to recognise trending topics and automatically categorise user-submitted posts, recommendations to provide users with fresh content to engage with, and finally reputation scoring to allow users to quantify content and user trustworthiness.

\subsection{Abuse Detection}
Abuse detection will be performed on Fidelis to filter out derogatory and abusive content from the users post feed. Performing abuse detection will provide users with confidence that they will not encounter content they deem offensive whilst on Fidelis. Different users will have varying levels of tolerance towards abuse, so users should be allowed to determine what abuse is filtered from their feed. To this end, users should be able toggle their `tolerance' level, which will determine what content is removed or included on their feed. The following sections will look at the various stages that need to be considered for abuse detection. Namely, data pre-processing, model building and data handling

\subsubsection{Data Pre-Processing}
As mentioned in Chapter \ref{Chapter:Research}, abuse detection is regularly achieved by building a classifier trained on features extracted from relevant texts. Before a classifier can be trained, the data must first be fetched and cleaned. Cleaning data is important before processing as `dirty' data can have an adverse effect on model performance. In the case of natural lanuage processing, data cleaning can involve \cite{han2011data}:
\begin{itemize}
\item Removing stop words such as ``the'' or ``a'', 
\item Removing punctuation, although it should be noted that certain punctuation should be kept with the original text, especially when performing sentiment analysis
\item Converting text to be all lower or upper-case
\item Removing HTML mark-up
\item Removing apostrophes
\end{itemize}

When building a model for text classification, the model is trained on significant features extracted from the text. These features are elements from the text, and common techniques used when extracting features during natural language processing are discussed in Chapter \ref{Chapter:Research}. As part of pre-processing, features must first be extracted from the data before the model is trained.

\subsubsection{Model Building}
Abuse detection is a classification problem; a piece of text is either abusive or not. A variety of machine learning classifiers can be used for this, but for our purposes the Support Vector Machine (SVM) and Logistic Regression (LR) models will be used. SVMs have been shown to be one of the best performing models for text classification \cite{joachims1998text}. This has also been shown for LR models \cite{genkin2007large}, with performance for these models being shown to be ``at least as effective as those produced by support vector machine classifiers''. Models will be built by training them on the set of features extracted from the data. To ensure that the produced classifier does not overfit the training data, and instead generalises well to the plethora of unseen data it will receive in the form of user posts cross-validation will be used. Cross-validation is a training technique that serves this purpose, and splits a dataset into separate training and testing sets \cite{scikit:cross-val}. Model performance is dependent on features extracted from data, and additionally on the parameters chosen for the model, which in turn are dependent on the data itself. To maximise model performance, parameters must be tuned to assess their suitability for the model and given data. Algorithm \ref{alg:abuse} provides a very high-level overview of the process of building a model for text classification.

\begin{algorithm}
\caption{Algorithm for training model}
\label{alg:abuse}
\begin{algorithmic}[1]
\State $data\gets getTwitterData$
\State $train\gets getTrainingData(data)$
\State $test\gets getTestData(data)$
\State $cleanData\gets cleanData(data)$
\State $features\gets extractFeatures(data)$
\State $classifier\gets trainModel(features)$
\State Test $classifier$ using $test$
\State Save $classifier$
\end{algorithmic}
\end{algorithm}

\subsubsection{Handling Abusive Data}
-On Facebook, 55 million status updates are made daily \cite{kissmetrics:fb-stats}, or $\tilde{636}$. It would therefore be infeasible to expect to categorise posts during real-time due to the sheer number of posts that can be encountered. To counteract this, the classifier should be run on posts daily at a chosen time. 
Algorithm \ref{alg:abuse-classification} shows the procedure that will be used to classify user posts made on Fidelis, and algorithm \ref{}
\begin{algorithm}
\caption{Algorithm for post classification}
\label{alg:abuse-classification}
\begin{algorithmic}[1]
\State $clf\gets loadClassifier$
\State $posts\gets getAllUserPosts$
\For{$post$ in $posts$}
	\State Classify $post$ using $clf$
	\If{$post$ is abusive}
		\State $author\gets getPostAuthor(post)$
		\State Update reputation score for $post$ and $author$
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Algorithm for post classification}
\label{alg:abuse-retrain}
\begin{algorithmic}[1]
\State $clf\gets loadClassifier$
\State $reports\gets getReportedPosts$
\State $cleanReports\gets cleanData(reports)$
\State $reportFeatures\gets extractFeatures(cleanReports)$
\State Re-train $clf$ using $reportFeatures$
\end{algorithmic}
\end{algorithm}

\subsection{Content Filtering}
The filtering of content allows the users to receive tailored content, which is more likely to be of interest of them and therefore instils trust in the social network. The importance of the performance of this area of the system on the success of the overall project means that three different models will be designed and compared using the techniques discussed in section \ref{sec:research-content-filtering}: Na\"ive Bayes, SVMs and k-nearest neighbours. Once the models are able to successfully detect the category of a post, the post category can be updated in the database, which will allow filtered feeds to retrieve the relevant content to be displayed to the user. 

This section will discuss the design decisions made for the content filtering throughout each stage of the process. This includes the preprocessing of the data and the training of the model as well as algorithms for testing the efficacy of the model and using it to predict new posts as they are made on Fidelis.

\subsubsection{Preprocessing}
Before being able to train the model, preprocessing must be carried out on the data. This is to remove any parts of the dataset which are not informative, thus reducing the size of the data, and also to construct new features which may aid in the building of the model. The algorithm \ref{alg:category-filter-preprocess} takes a single post and then returns a multiset $features$ containing each of the features in the post (the same feature may appear multiple times in a single post). 

\begin{algorithm}
\caption{Category filter preprocessing algorithm}
\label{alg:category-filter-preprocess}
\begin{algorithmic}[1]
\Function{preprocess}{Post $post$}
	\State $features\gets \emptyset$
	\State $post\gets post\setminus nonAsciiChars(post)$ 
	\State $post\gets post\setminus cleanTwitterEntities(post)$
	\State $post\gets post\setminus removePunc(post)$
	\For{\textbf{each} $word$ \textbf{in} $post$}
		\State $word\gets lemmatize(word)$
		\If{$word \in stopwords$} 
			\State \textbf{break} 
		\EndIf
		\If{$len(word) < 3$} 
			\State \textbf{break} 
		\EndIf
		\State Add $word$ to $features$
	\EndFor
	\State $features\gets features\cup bigrams(features)\cup trigrams(features)$
	\State \Return{$features$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Building the Models}


\subsubsection{Predicting Categories}

\subsection{Recommendations}
Recommendations are a key component to Fidelis, and this section will look at the design of the algorithms for this purpose. Collaborative-based filtering was chosen as the filtering technique for recommendations. Justification for this is apparent in the technique itself; collaborative-filtering focuses on using the opinions of like-minded users, coupled with items rated by the user in the past. This sets up recommendations for success as they are much more likely to be greeted positively by the user. Given the nature of Fidelis as social network, resources in terms of previously rated items (through votes on posts) and like-minded users will be available in abundance to generate strong recommendations. It is important to provide the user with choice for how they would like to receive recommendations. To this end, the user will be provided with the option to choose how their recommendations should be provided. The following sections will look at the algorithm that will be used to generate recommendations for users, and will also discuss three different recommendation flavours that will be offered to the user, giving them final say on how their recommendations are generated. 

\subsubsection{User Recommendations}
\label{sec:user-recommendations}
Users are likely to interact with recommendations they receive on a daily basis. As such, algorithms generating recommendations should run on a daily basis. To generate recommendations, we must therefore first look at the number of recommendations already generated for each user. A user may choose not to interact with provided recommendations, so new recommendations should be made only when a user has approved or rejected their current recommendations. For each user, their item vector should be retrieved. The item vector will correspond to the features discussed in Chapter \ref{Chapter:Research}. The set of candidate recommendations for the user will be determined by what recommendation method they have chosen. Before checking for similarity between the user and the candidate recommendations, all users the user already follows, has blocked or has rejected as a recommendation should be removed from the candidate set. In the event that there are no candidate users, a default set of recommendations will be provided for the user. Using the retrieved collection of candidate recommendations, each user in this collection will be assessed to determine whether the similarity between them and the user is sufficiently large. Suitability between users will be determined using a threshold value. We are only interested in similarities that are at least as large as this value, as similarities less than the threshold value would mean that the two users in question are not similar. Only those users with a similarity greater than the threshold value should be provided as a recommendation. Similarity will be measured using cosine similarity. Cosine similarity is calculated using the following equation:
\begin{equation}
sim(\mathbf{A},\mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{\parallel\mathbf{A}\parallel \parallel\mathbf{B}\parallel} = = \frac{ \sum\limits_{i=1}^{n}{A_i  B_i} }{ \sqrt{\sum\limits_{i=1}^{n}{A_i^2}}  \sqrt{\sum\limits_{i=1}^{n}{B_i^2}} }
\label{eq:cos-similarity}
\end{equation}
Algorithm \ref{alg:user-recommendations} provides pseudocode for this process. Values for $similarityThreshold$ and $val$ should be set by the user.

\begin{algorithm}
\caption{User recommendations algorithm}
\label{alg:user-recommendations}
\begin{algorithmic}[1]
\State $similarityThreshold\gets 0.7$
\State $val\gets 5$
\ForAll{users $u$}
	\State $method\gets getRecommendationMethod(u)$
	\State $currentRecommendations\gets getNumberOfRecommendations(u)$
	\If{$currentRecommendations < val$}
		\State $uVector\gets getItemVector(u)$
		\If{$method = Friend$-$of$-$a$-$Friend$}
			\State $users\gets getFOFUsers(u)$
		\EndIf
		\If{$method = Explorer$}
			\State $users\gets getExplorerUsers(u)$
		\EndIf
		\If{$method = Hybrid$}
			\State $fof\gets getFOFUsers(u)$
			\State $explorer\gets getExplorerUsers(u)$
			\State $users\gets fof \cap explorers$
		\EndIf
		
		\State $uFollowing\gets getFollowing(u)$
		\State $uBlocked\gets getBlockedUsers(u)$
		\State $uRejected\gets getRejectedRecommendations(u)$
		\State $users = users \setminus uFollowing \setminus uBlocked \setminus uRejected$
		
		\If{$\left\vert{users}\right\vert = 0$}
			\State $getDefaultUsers$
		\Else
			\For{$v$ in $users$}
				\State $vVector\gets getItemVector(v)$
				\State $similarity\gets measureSimilarity(uVector, vVector)$
			
				\If{$similarity \geq similarityThreshold$}
					\State $storeRecommendation(v)$
				\EndIf
			\EndFor
		\EndIf
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

We will now look at each of the possible user sets for candidate recommendations. In particular, we will observe a general overview of the method for user selection and involves ourselves with a discussion on the method itself with regards to the wider system.

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{.3\linewidth}
		\includegraphics[height=1.7in]{Images/Design/facebook}
		\caption{}
		\label{fig:fof-facebook}
	\end{subfigure}
	\begin{subfigure}[b]{.5\linewidth}
		\includegraphics[width=1\linewidth]{Images/Design/linkedin}
		\caption{}
		\label{fig:fof-linkedin}
	\end{subfigure}
	\caption{Friend-of-a-Friend recommendations from (a) Facebook and (b) Twitter}
	\label{fig:fof}
\end{figure}

\paragraph{Friend-of-a-Friend Users}
With this method, candidate recommendations for a user $A$ are all users $C$ where $A$ follows $B$ and $B$ follows $C$. This method uses like-minded users for recommendations, and exploits the transitive nature of the ``following'' relationship. It is a popular technique used by numerous social networks, including Facebook (Figure \ref{fig:fof-facebook}) and Linkedin (Figure \ref{fig:fof-linkedin}). This option will allow the user to generate ``safer'' recommendations by using the idea that ``if $A$ trusts $B$, and $B$ trusts $C$, $A$ will most likely also trust $C$''. This is not always the case, but mostly holds true. It is important to give users this choice as it provides them more control over who enters their trust circle. Although one of the project aims is to burst opinion bubbles through exposure to different and trusted views, users must still be provided with this option. The function in Algorithm \ref{alg:fof-users} provides a procedure for retrieving ``$C$'' users. 

\begin{algorithm}
\caption{Function for getting Friend-of-a-Friend users}
\label{alg:fof-users}
\begin{algorithmic}[1]
\Function{getFOFUsers}{User u}
	\State $users = \emptyset$
	\State $uFollowing\gets getFollowing(u)$
	\For{$f$ in $uFollowing$}
		\State $fFollowing\gets getFollowing(f)$
		\State Add $fFollowing$ to $users$
	\EndFor
	\State \Return{$users$}		
\EndFunction
\end{algorithmic}
\end{algorithm}

\paragraph{Explorer Users}
Fidelis aims to expose users to opinions and users they may not initially interact with. Explorer users are chosen for exactly this reason. Candidate recommendations are found by finding users' favourite tags to post in and looking at other users who post in the same tag. This user collection method combines the users rated items and looks at like-minded users. By deviating from the users trust circle, we are more likely to generate recommendations that will broaden user opinions by finding other users who care about the same topics, but will naturally hold different views on them. This method seeks to expand a users trust circle beyond current opinions held by them. The function in Algorithm \ref{alg:explorer-users} shows the procedure for collecting Explorer users. $N$ is used to determine the number of tags that will be searched for candidate recommendations. The function in Algorithm \ref{alg:explorer-users} shows this procedure.

\begin{algorithm}
\caption{Function for getting Explorer users}
\label{alg:explorer-users}
\begin{algorithmic}[1]
\Function{getExplorerUsers}{User u}
	\State $threshold\gets 75$
	\State $users\gets \emptyset$
	\State Get $N$ top tags $u$ posts in
	\For{$tag t = 1$ to $N$}
		\State Get all users $v$ posting in $t$
		\If{$reputationOf(v) >= threshold$}
			\State Add $v$ to $users$
		\EndIf
	\EndFor
	\State \Return{$users$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\paragraph{Hybrid Users}
Using this method aims to find the middle ground between the two aforementioned techniques. The intersection between Explorer and Friend-of-a-Friend users, as shown in Figure \ref{fig:hybrid}, is returned as the set of candidate recommendations. 

\begin{figure}
\centering
\begin{tikzpicture}[
    thick]
    \draw [fill=cyan, fill opacity=0.5, name path=c1] (0,0) circle (2cm);
    \draw [fill=orange, fill opacity=0.5, name path=c2] (3,-1) circle (2.5cm);
    \draw (0,0) ++(120:2cm) -- ++(120:2.2cm) node [fill=white,inner sep=5pt](a){Friend-of-a-Friend};
    \draw (3, -1) ++(30:2.5cm) -- ++(30:2.6cm) node [fill=white,inner sep=5pt](b){Explorer};
    \path [name intersections={of=c1 and c2,by=cs}];
    \draw (cs) -- ++(.5,1) node [fill=white,inner sep=5pt](c){Hybrid};
\end{tikzpicture}
\caption{Hybrid user selection}
\label{fig:hybrid}
\end{figure}

\paragraph{Default Recommendations}
If all aforementioned methods fail to return a set of candidate recommendations, the system should revert to default candidate recommendations. This method will return either the $N$ most popular or reputable users on Fidelis. The function in Algorithm \ref{alg:default-users} shows this procedure for most popular users, but the same procedure can be re-used for getting the most reputable users.

\begin{algorithm}
\caption{Function for getting default users}
\label{alg:default-users}
\begin{algorithmic}[1]
\Function{getDefaultUsers}{User u}
	\State $users\gets \emptyset$
	\State $users\gets getMostPopularUsers$
	\State $users\gets sortDescending(users)$
	\State \Return{$N$ top users from $users$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Content Recommendations}
\label{sec:content-recommendations}
Similarly to user recommendations, users will interact with content recommendations made for them daily and so new content recommendations should be generated for them daily also. Again, before any new recommendations are made we must look at how many recommendations the user currently has. By doing this, we limit any unnecessary computation. In the case of content recommendations, user item vectors will not be used. It was instead decided to use the reputation of a given post, rather than the similarity between the user recommendations are being generated for and the author of the post. The justification behind this is that user recommendations should focus on locating like-minded users, whereas content recommendations should avoid this to diversify the content made available to users. Providing recommendations in this manner again emphasises the desire for Fidelis to break the constraints of the ``echo chamber effect'' currently restricting most social media platforms. By letting the user pick a reputation threshold for recommendations generated for them, we can ensure that users trust their recommendations and can confidently interact with them knowing they are trustworthy. Algorithm \ref{alg:content-recommendations} provides the pseudocode for the procedure described above. 

\begin{algorithm}
\caption{Content recommendations algorithm}
\label{alg:content-recommendations}
\begin{algorithmic}[1]
\State $val\gets 5$
\State $reputationThreshold\gets 0.7$
\ForAll{users $u$}
	\State $method\gets getRecommendationMethod(u)$
	\State $currentRecommendations\gets getNumberOfRecommendations(u)$
	\If{$currentRecommendations < val$}
		\If{$method = Friend$-$of$-$a$-$Friend$}
			\State $posts\gets getFOFPosts(u)$
		\EndIf
		\If{$method = Explorer$}
			\State $posts\gets getExplorerPosts(u)$
		\EndIf
		\If{$method = Hybrid$}
			\State $fof\gets getFOFPosts(u)$
			\State $explorer\gets getExplorerPosts(u)$
			\State $posts\gets fof \cap explorers$
		\EndIf
		
		\State $uVoted\gets getVotedPosts(u)$
		\State $uBlocked\gets getBlockedPosts(u)$
		\State $posts = posts \setminus uVoted \setminus uBlocked$
		
		\If{$\left\vert{posts}\right\vert = 0$}
			\State $getDefaultContent$
		\Else
			\For{$p$ in $posts$}
				\If{$getReputation(p) \geq reputationThreshold$}
					\State $storeRecommendation(p)$
				\EndIf
			\EndFor
		\EndIf
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

We will again look at how each of the four recommendation techniques will generate candidate post sets. By providing the user with different ways recommendations can be generated for them will again ensure that users are in control of content they will interact with. 

\paragraph{Friend-of-a-Friend content} Candidate recommendations using this method will build on the follows relationship mentioned earlier in Chapter \ref{sec:user-recommendations} by retrieving the posts from Friend-of-a-Friend users. Getting content in this manner will provide users with a selection of posts that are likely to agree with their own sentiments. 

\begin{algorithm}
\caption{Function for getting Friend-of-a-Friend content}
\label{alg:fof-content}
\begin{algorithmic}[1]
\Function{getFOFUsers}{User u}
	\State $posts\gets \emptyset$
	\State $uFollowing\gets getFollowing(u)$
	\For{$f$ in $uFollowing$}
		\State $fFollowing\gets getFollowing(f)$
		\For{$g$ in $fFollowing$}
			\State $gPosts\gets getPosts(g)$
			\State Add $gPosts$ to $posts$
		\EndFor  
	\EndFor
	\State \Return{$posts$}		
\EndFunction
\end{algorithmic}
\end{algorithm}

\paragraph{Explorer content} Explorer content recommendations stress the idea previously mentioned about removing the constraints of the ``echo chamber effect''. By considering other users who post in the same categories as the user recommendations are being generated for, we only consider posts that the user will be interested in. Without considering the similarity between the authors of these posts and the user themselves, we increase the likelihood of exposing users to content they would not normally interact with.

\begin{algorithm}
\caption{Function for getting Explorer content}
\label{alg:explorer-content}
\begin{algorithmic}[1]
\Function{getExplorerPosts}{User u}
	\State $posts\gets \emptyset$
	\State $users\gets getExplorerUsers(u)$
	\For{$u$ in $users$}
		\State $uPosts\gets getPosts(u)$
		\State Add $uPosts$ to $posts$
	\EndFor
	\State \Return{$posts$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\paragraph{Hybrid content} Using this method aims to find the middle ground between the two aforementioned techniques. We take a similar approach as to before and return the intersection between Explorer and Friend-of-a-Friend content as the set of candidate recommendations.

\paragraph{Default content} Default content recommendations will return the $N$ most popular or reputable posts made on Fidelis. . The function in Algorithm \ref{alg:default-content} shows the procedure for retrieving the most popular posts, but the same procedure can be re-used for getting the most reputable posts.

\begin{algorithm}
\caption{Function for getting default content}
\label{alg:default-content}
\begin{algorithmic}[1]
\Function{getDefaultContent}{}
	\State $posts\gets \emptyset$
	\State $posts\gets getMostPopularPosts$
	\State $posts\gets sortDescending(posts)$
	\State \Return{$N$ top posts from $posts$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Reputation Scoring}
It seems clear that collaboration events are a popular method of assessing reputation in online systems. The Sporas and Histos algorithms demonstrate mechanisms for this scoring using collaboration events \cite{mcnally2013} \cite{zacharia2000}. Ultimately, however, these algorithms did not meet the requirements for reputation scoring. The Fidelis system would in theory serve a very complex network of users, which constrains the choice of reputation scoring algorithms. The proposed algorithm would need to run on the entire network each day (to score the reputation of both users and posts). Histos and Sporas would both be costly to run given the data stored in our database and so a similar, less complex method that adopts elements from Histos and Sporas was considered. This method involves simply performing a sum of each collaboration event, where each event type has a different weighting denoting the relative `importance' of the event in calculating reputation. The weighting functions are shown here for users and posts respectively

\begin{equation}
	\label{eq:rep_weight_user}
		w(c_u) = \left\{\begin{matrix}
			0.3 & if\ c\ is\ an\ upvote\ on\ a\ post\ by\ u\\ 
			-0.3 & if\ c\ is\ a\ downvote\ on\ a\ post\ by\ u \\ 
			0.1 & if\ c\ is\ an\ upvote\ on\ a\ comment\ by\ u \\ 
			-0.1 & if\ c\ is\ a\ downvote\ on\ a\ comment\ by\ u \\ 
			0.2 & if\ c\ is\ a\ comment\ on\ a\ post\ by\ u\\ 
			0.5 & if\ c\ is\ another\ user\ following\ u
		\end{matrix}\right.
\end{equation}
		
\begin{equation}
	\label{eq:rep_weight_post}
	 w(c_p) = \left\{\begin{matrix}
			0.1 & if\ c\ is\ an\ upvote\ on\ post\ p \\ 
			-0.1 & if\ c\ is\ a\ downvote\ on\ post\ p \\ 
			0.2 & if\ c\ is\ a\ comment\ on\ post\ p
	\end{matrix}\right.
\end{equation}

\noindent
, where the weighting function \(w\) gives the weighting of a collaboration event involving user \(u\): \(c_u\). The values of this function could, of course, be changed at any time in an attempt to produce more accurate reputation scoring. Once a reputation score has been calculated for each user, these scores will be scaled between 0 and 1.

\section{Database}

\section{User Interface}

\section{Responsive Design}
\chapter{Testing}
\label{Chapter:Testing}

\section{Unit Testing}
Unit testing is a software development process in which the smallest testable parts of an application, called units, are individually and independently scrutinised for proper operation \cite{TechTarget:UnitTesting}. Through unit testing, the developer can test each aspect of the system at a micro level before each component is integrated into the system. These unit tests were mostly dictated by the functional and non-functional requirements of the system to ensure that the component satisfied all the requirements. Throughout this process, white-box testing has been used along with some black-box testing to ensure expected behaviour is provided by the system. Table \textbf{TABLE} below discusses the tests carried out as part of unit testing by stating the expected behaviour and the actual behaviour.

\section{Integration Testing}
Integration testing is a logical extension of unit testing. In its simplest form, two units that have already been tested are combined into a component and the interface between them is tested \cite{MSDN:IntegrationTesting}. As the major functionality of each component was completed, the component could now be integrated into the system so that it may work along the other components. Once integrated, integration testing could be carried out to ensure that each individual component functioned alongside other components, that it may have relied on, without resulting in errors. Integration testing prevents errors from propagating into the subsequently implemented components. Every time a component was integrated, all the unit tests for that components were run again to verify its functionality. Any errors that occurred were fixed and the tests were rerun before moving onto the next component.

\section{System Testing}
System testing is most often the final test to verify that the system to be delivered meets the specification and its purpose \cite{ISTQB:SystemTesting}. In system testing the behaviour of whole system is tested as defined by the scope of the development project or product \cite{ISTQB:SystemTesting}. However, system testing was carried out for the project as the system was developed as well as at the end - This means that as each component was developed and integrated, thorough testing was done on the component integrated, any other components it affected as well as the general functionality of the system. In comparison to unit and integration testing which verify that functional requirements are met, system testing verifies both functional and non-functional requirements of the system. Additionally, at the end of the development process, a comprehensive and thorough system test was carried out using multiple user accounts. These tests followed the same convention and produced the same results as the unit tests. Any errors that occurred were patched on the go and the testing was done all over again.

\subsection{Validation Testing}
\subsection{Permission Testing}

\section{User Acceptance Testing}

\subsection{User Feedback}
The entire system relies on a stable and growing user base; without the users the system would be rendered useless. With this in mind, guaranteeing a more than satisfactory user experience was an integral system success, and it was therefore important to gain user feedback on the user experience offered by Fidelis. By correctly analysing feedback received from users, system strengths and weaknesses could be used to identified for each progressive system prototype. A number of users were consulted after being given access to the system at various stages of development in order to identify any issues that users may have with the system so that these could be dealt with in latter development stages.

At various stages of the system the users were given surveys along with a list of tasks to complete. Once the users completed each task, they would answer the questions corresponding to the tasks. No guidance was given to the user and they were expected to use their intuition to navigate the system and perform the tasks. The survey not only included questions on system navigation, but also had questions related to UI design, recommendations and 

As mentioned previously, Fidelis design adopted a lot of elements from Twitter. As a result of this, users felt very comfortable navigating through and using Fidelis. Feedback received, mostly orally, highlighted that the system did need to re-design elements of the UI as it felt and looked too similar to Twitter. A discussion of UI re-design is included in Chapter \ref{Chapter:Conclusion}. Users were very receptive towards the idea of controlling how recommendations were generated for them. By being able to specify a method for recommendations, users were enabled to buy into the idea of Fidelis being a platform that enables them to engage with content they are interested in and have control over. Similar sentiments were voiced on the ability to fine-tune how abuse detection and reputation scoring was conducted.

Acceptance testing played a key role in the development process of the system. Prototypes were built and presented to the user which they evaluated and the feedback from the users was then incorporated into the next iteration of the users. Often a different selection was used to provide an unbiased feedback as existing users would be familiar with the system after previous tests. However, on the final acceptance testing of the complete system, a mix of new and existing users were requested to test the system to see what new users thought about the final product as well as how existing users thought the system had improved.

\subsection{Testimonials}

\section{System Tuning and Assessment}
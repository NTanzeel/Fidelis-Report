\subsection{Content Filtering}
The implementation of content filtering constitutes of Python scripts which handle all of the data preprocessing, training of the model and predicting of the post categories. The same Python libraries used for abuse detection are used here. The code is divided between three files, each of which handle a stage of the machine learning process: \emph{preprocess.py}, \emph{train.py} and \emph{predict.py}.

\subsubsection{Data Description}
Although in the future, Fidelis posts will be used to train the model, until there is sufficient data stored a training set from Twitter is used, as discussed in section \ref{sec:training-data}. This training set contains two attributes: tweet text and category. The categories correspond to the default main categories which are used on the discover page, which are: Arts, Business, Education, Games, Health, Home, News, Recreation, Science, Shopping, Society, Sport and Technology. There are 59010 tweets in the dataset. The graph in figure \ref{fig:category-spread} shows the distribution of tweets in each category, with Arts being the most prevalent with 6680 tweets and Shopping the least with 693 tweets.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/Implementation/category-spread}
\caption{Spread of Tweets over categories in training set}
\label{fig:category-spread}
\end{figure}

\subsubsection{Data Preprocessing}
\textit{preprocess.py} cleans posts so that they can be used in either the training or predicting processes. The implementation of cleaning is shown in figure \ref{fig:content-clean}. The function carries out a list of commands sequentially, each of which remove aspects of the post which are unwanted. The package \textit{p}, from which the \textit{clean()} function is used, is the tweet-preprocessor package \cite{TweetPreprocessor}. This function removes any Twitter entities, except for the hashtags, from the post, including mentions, URLs and emoticons. Since Fidelis is using the same tagging system as Twitter, the same cleaning function can be applied to Fidelis posts. The $WordLemmatizer$ object is from the NLTK library, which processes lemmatized versions each of the words in the post to reduce the noise created as a result of different forms of the same word. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/Implementation/content-clean}
\caption{Function that cleans a post}
\label{fig:content-clean}
\end{figure}

The only part of algorithm \ref{alg:content-filter-cleaning} not performed by this function is the check on the length of the preprocessed string. This is instead performed by both the training and prediction functions. Figure \ref{fig:content-preprocess} shows the preprocessing which occurs in the training file.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/Implementation/content-preprocess}
\caption{Preprocessing of content before training}
\label{fig:content-preprocess}
\end{figure}

The extraction of features from the post is conducted in the training and prediction files, because they are performed on the dataset as a whole rather than on each individual post. The count vectorization and TF-IDF transformations are parsed into the pipeline shown in figure \ref{fig:imp-content-train}, using the scikit-learn objects $CountVectorizer$ and $TfidfTransformer$. The $CountVectorizer$ also creates the uni- and bi-grams which are present in the dataset.

\subsubsection{Model Building}
Figure \ref{fig:imp-content-train} shows how the Stochastic Gradient Descent model is trained, including the pipeline which extracts features from the dataset. Whereas the pipeline stage creates the model, the \textit{fit()} function is what initiates the training of the model on the training data. As in section \ref{sec:exporting}, the trained models can then be exported so that they are usable for predicting incoming tags and posts.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/Implementation/content-train}
\caption{Training the model}
\label{fig:imp-content-train}
\end{figure}

However, as well as building the model using the entire dataset, cross-validation is also implemented. Figure \ref{fig:content-cv} shows how cross-validation is performed. This is done alongside the training of the model in \emph{train.py}. The function \textit{cv\_val\_score()} is imported from scikit-learn, and handles the splitting of the dataset, and the training and testing of each model.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/Implementation/content-cv}
\caption{Cross-validating the model}
\label{fig:content-cv}
\end{figure}

The graph in figure \ref{fig:cv-results} shows the accuracy of each model for each iteration of the 10-fold cross-validation. SGD produces the best performing model, with an average accuracy of 0.606 and therefore would be a sensible choice of model for Fidelis. The SVM performs the worst, however, the accuracy of the models could be improved in the future, by fine tuning model parameters. In addition, the cross-validation is performed on full posts, and therefore the accuracy when the model is used for predicting the category of tags can be evaluated separately, when there is a sufficiently large dataset to test.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/Implementation/cv-results}
\caption{Cross-validation results for each model}
\label{fig:cv-results}
\end{figure}

\subsubsection{Predicting Topics}
The categories of the tag are predicted every 15 minutes by the execution of a scheduled program. This Python script, which is in figure \ref{fig:predict-tag} collects all the tags from the \emph{tags} table where the \emph{categorised} attribute is set to 0. These uncategorised tags are then passed to the imported classification model, which predicts which category the tag belongs to. Finally, the database is updated and the category-tag pairs are added to the \emph{category\_tags} table so that they can be used by the discover page. Interactions with the database are handled using a MySQL cursor from the \emph{mysql} package. The cursor is open throughout the execution of the script until it is closed at the end of the prediction process. All of the tags collected from the database are stored in a Pandas dataframe so that they can be passed to the machine learning model. Furthermore, the \emph{category.csv} file is imported, which contains the mappings from the integer values representing the categories in the trained model and the actual name of the category.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/Implementation/predict-tag}
\caption{Predicting the tag's category}
\label{fig:predict-tag}
\end{figure}

Post categories are predicted using the API, which makes a call to the \emph{PostController}'s \textit{predict()} function after a post has been made. As shown in figure \ref{fig:predict-api}, the function attaches the post to the tag which represents the category. Because the model used to predict the post is a Python script, the \emph{PostController} uses the $Process$ object to run the script and retrieve the predicted category which is returned by the script. After attaching the post to the tag, and thus adding the post-tag pair to the \emph{post\_tag} table, the API returns the post's category so that JavaScript can be used to update the category on the user's display. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/Implementation/predict-api}
\caption{API controller for predicting a post}
\label{fig:predict-api}
\end{figure}

The code in figure \ref{fig:predict-post} shows the Python script which is used to predict the category of the post. As with figure \ref{fig:predict-tag}, the code is stored in the \emph{predict.py} file, but the two sections are distinguished by whether a post ID argument is passed when executing the script or not. This part of the script does not use MySQL cursors because any processing involved with the database is handled in the \emph{PostController}. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/Implementation/predict-post}
\caption{Predicting a post's category}
\label{fig:predict-post}
\end{figure}

Once the post has been categorised, the user would then be able to change this category in the event that the post has been misclassified. This may be the case when the post does not belong to a category, because the model will always predict that the post belongs to one of the 13 categories even if it is miscellaneous (unless the preprocessed post contains less than 20 characters). The updating of the category is performed by the API once the user has selected a new category using the Bootstrap modal shown in figure \ref{fig:category-modal}. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/Implementation/category-modal}
\caption{Modal to edit post category}
\label{fig:category-modal}
\end{figure}

The API request passes the relevant post and (new) category IDs to the \textit{editCategory()} function in the PostController, which is shown in figure \ref{fig:edit-category}. This function removes the old category from the post\_tags table and replaces it with the new one. The API then returns the new category name to the application front-end so that the changes can be reflected by the user interface. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/Implementation/edit-category}
\caption{API controller for changing a post's category}
\label{fig:edit-category}
\end{figure}

